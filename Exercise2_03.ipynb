{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.03: Preparing the Quandl Data for Prediction\n",
    "\n",
    "The goal of this exercise is to download an external dataset from the Quandl library and then prepare it so that it is ready for use in our linear regression models.\n",
    "\n",
    "The following steps will help you to complete this exercise:\n",
    "\n",
    "  > **Note**  \n",
    "  > You need to install Qaundl library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.- Download the data into a DataFrame using Quandl for the S&P 500 between 1950 and 2019. Its ticker is `“YALE/SPCOMP”`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The column index must be expressed as an integer for ../syllabus_intelligencesystems/data/YALE_SPCOMP_test.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-72bbaf2480e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquandl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../syllabus_intelligencesystems/data/YALE_SPCOMP_test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\quandl\\get.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(dataset, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m# Unicode String\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mdataset_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_dataset_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdataset_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'column_index'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'column_index'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdataset_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'column_index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\quandl\\get.py\u001b[0m in \u001b[0;36m_parse_dataset_code\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mdataset_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdataset_temp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mERROR_COLUMN_INDEX_TYPE\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'code'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdataset_temp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'column_index'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_temp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The column index must be expressed as an integer for ../syllabus_intelligencesystems/data/YALE_SPCOMP_test.csv."
     ]
    }
   ],
   "source": [
    "import quandl\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "data_frame = quandl.get('../syllabus_intelligencesystems/data/YALE_SPCOMP_test.csv')\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.- Use the `head()` method to visualize the columns inside the `data_frame.head()` DataFrame.\n",
    "\n",
    "Output:\n",
    "\n",
    "![Figure 2.18](img/fig2_18.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame[['Long Interest Rate', 'Real Price', 'Real Dividend']]\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few features seem to highly correlate with each other. For instance, the `Real Dividend` column grows proportionally with `Real Price`. The ratio between them is not always similar, but they do correlate.\n",
    "\n",
    "As regression is not about detecting the correlation between features, we would rather get rid of the features that we know are correlated and perform regression on the features that are non-correlated. In this case, we will keep the `Long Interest Rate`, `Real Price`, and `Real Dividend` columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.- Keep only the relevant columns in the `Long Interest Rate`, `Real Price`, and `Real Dividend` DataFrames:\n",
    "\n",
    "Output:\n",
    "\n",
    "![Figure 2.19](img/fig2_19.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.fillna(method='ffill', inplace=True)\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the DataFrame contains a few missing values `NaN`. As regression doesn't work with missing values, we need to either replace them or delete them. In the real world, we will usually choose to replace them. In this case, we will replace the missing values by the preceding values using a method called **forward filling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.- Replace the missing values with a forward filling.\n",
    "\n",
    "Output:\n",
    "\n",
    "![Figure 2.20](img/fig2_20.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['Real Price Label'] = data_frame['Real Price'].shift(-3)\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleaned the missing data, we need to create our label. We want to predict the `Real Price` column 3 months in advance using the current `Real Price`, `Long Interest` Rate, and `Real Dividend` columns. In order to create our label, we need to shift the `Real Price` values up by three units and call it `Real Price Label`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.- Create the `Real Price Label` label by shifting `Real Price` by 3 months.\n",
    "\n",
    "Output:\n",
    "\n",
    "![Figure 2.21](img/fig2_21.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-cb0f00c9873c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Real Price Label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mscaled_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscaled_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'drop'"
     ]
    }
   ],
   "source": [
    "features = np.array(data_frame.drop('Real Price Label', 1))\n",
    "scaled_features = preprocessing.scale(features)\n",
    "scaled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The side effect of shifting these values is that missing values will appear in the last three rows for `Real Price Label`, so we need to remove the last three rows of data. However, before that, we need to convert the features into a NumPy array and scale it. We can use the `drop` method of the DataFrame to remove the label column and the preprocessing function from `sklearn` to scale the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.- Create a NumPy array for the features and scale it.\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "array([[-1.14834808, -1.13030753, -1.19145337],\n",
    "       [-1.14109376, -1.12504171, -1.17960932],\n",
    "       [-1.13383944, -1.12398084, -1.17363711],\n",
    "       ...,\n",
    "       [-1.36075461,  2.93924962,  3.67194058],\n",
    "       [-1.32593387,  3.12706161,  3.7120735 ],\n",
    "       [-1.3085235 ,  3.23602381,  3.75460521]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features_latest_3 = scaled_features[-3:]\n",
    "scaled_features = scaled_features[:-3]\n",
    "scaled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `1 in the second argument specifies that we are dropping columns. As the original DataFrame was not modified, the label can be directly extracted from it. Now that the features are scaled, we need to remove the last three values of the features as they are the features of the missing values in the label column. We will save them for later in the prediction part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.- Remove the last three values of the features array and save them into another array\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "array([[-1.14834808, -1.13030753, -1.19145337],\n",
    "       [-1.14109376, -1.12504171, -1.17960932],\n",
    "       [-1.13383944, -1.12398084, -1.17363711],\n",
    "       ...,\n",
    "       [-1.07522452,  2.86136628,  3.47166907],\n",
    "       [-1.12049149,  2.78043332,  3.50007195],\n",
    "       [-1.23888201,  2.83151407,  3.54006023]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.dropna(inplace=True)\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scaled_features` variable doesn't contain the three data points anymore as they are now in `scaled_features_latest_3`. Now we can remove the last three rows with missing data from the DataFrame, then convert the label into a NumPy array using `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.- Remove the rows with missing data\n",
    "\n",
    "Output:\n",
    "\n",
    "![Figure 2.22](img/fig2_22.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.array(data_frame['Real Price Label'])\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.- Now let's see if we have accurately created our label.\n",
    "\n",
    "Output:\n",
    "\n",
    "![Figure 2.23](img/fig2_23.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our variable contains all the labels and is exactly the same as the `Real Price Label` column in the DataFrame.\n",
    "\n",
    "Our next task is to separate the training and testing data from each other. As we saw in the Splitting Data into Training and Testing section, we will use $90\\%$ of the data as the training data and the remaining $10\\%$ as the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.- Split the `features` data into training and test sets using `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "(features_train, features_test, label_train, label_test) = model_selection.train_test_split(scaled_features, label, test_size=0.1, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By completing this exercise, we have learned all the required steps for data preparation before performing a regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
